{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Search with  Libary BS4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import bs4\n",
    "# import requests\n",
    "# import pandas as pd\n",
    "\n",
    "# def scrape_google(keyword: str, page_count: int, base_url: str) -> pd.DataFrame:\n",
    "#     try:\n",
    "#         all_news = []\n",
    "#         NUMBER_PAGE = page_count\n",
    "#         query = keyword\n",
    "#         next_link = None  # Initialize next_link to None\n",
    "\n",
    "#         # Pagination\n",
    "#         for i in range(NUMBER_PAGE):\n",
    "#             if next_link is None:\n",
    "#                 # First page request\n",
    "#                 response = requests.get(base_url + \"search?q={\" + query + \"}&tbm=nws\")\n",
    "\n",
    "#                 if response.status_code != 200:\n",
    "#                     print('Error in Google URL')\n",
    "#                     continue\n",
    "\n",
    "#                 soup = bs4.BeautifulSoup(response.content, features=\"lxml\")\n",
    "#                 print(soup)\n",
    "#                 # print(soup)\n",
    "#                 # Extract link for the next page\n",
    "#                 next_link_element = soup.find('a', {'aria-label': 'Halaman berikutnya'})\n",
    "#                 # print(next_link_element)\n",
    "#                 if next_link_element:\n",
    "#                     next_link = next_link_element.get('href')\n",
    "#                 else:\n",
    "#                     print('No next page link found')\n",
    "#                     break\n",
    "#             else:\n",
    "#                 # Subsequent page requests\n",
    "#                 response = requests.get(base_url + next_link)\n",
    "#                 soup = bs4.BeautifulSoup(response.content, features=\"lxml\")\n",
    "#                 next_link_element = soup.find('a', {'aria-label': 'Halaman berikutnya'})\n",
    "#                 # print(next_link_element)\n",
    "#                 if next_link_element:\n",
    "#                     next_link = next_link_element.get('href')\n",
    "#                 else:\n",
    "#                     print('No next page link found')\n",
    "#                     break\n",
    "\n",
    "#             # Extract news data from the current page\n",
    "#             for element in soup.select(\"div > a\"):\n",
    "#                 if str(element.text).endswith('ago'):\n",
    "#                     news = {}\n",
    "#                     # Extracting link, title, source, and timestamp\n",
    "#                     news['link'] = str(element.get('href')).replace(\"/url?q=\", \"\")\n",
    "#                     news['title'] = element.select_one('h3').text\n",
    "#                     for child in element.find_parent().find_all('div', recursive=False):\n",
    "#                         if len(child.select('span')) == 5:\n",
    "#                             news['source'] = child.select('span')[3].text\n",
    "#                             news['timestamp'] = child.select('span')[4].text\n",
    "#                             break\n",
    "\n",
    "#                     # Adding additional information to the news\n",
    "#                     news['search_engine'] = 'google'\n",
    "#                     news['page_count'] = i+1\n",
    "#                     news['search_string'] = keyword\n",
    "#                     # Adding news dictionary to list\n",
    "#                     all_news.append(news)\n",
    "\n",
    "#         # Creating final DataFrame\n",
    "#         final_df = pd.DataFrame(all_news)\n",
    "#         return final_df\n",
    "#     except Exception as e:\n",
    "#         print('Error in Google:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import bs4\n",
    "# import requests\n",
    "# import pandas as pd\n",
    "\n",
    "# def scrape_google(keyword: str, page_count: int, base_url: str) -> pd.DataFrame:\n",
    "#     try:\n",
    "#         all_news = []\n",
    "#         NUMBER_PAGE = page_count\n",
    "#         query = keyword\n",
    "#         next_link = None  # Initialize next_link to None\n",
    "\n",
    "#         # Pagination\n",
    "#         for i in range(NUMBER_PAGE):\n",
    "#             if next_link is None:\n",
    "#                 # First page request\n",
    "#                 response = requests.get(base_url + \"search?q=\" + query + \"&tbm=nws\")\n",
    "\n",
    "#                 if response.status_code != 200:\n",
    "#                     print('Error in Google URL')\n",
    "#                     continue\n",
    "\n",
    "#                 soup = bs4.BeautifulSoup(response.content, features=\"lxml\")\n",
    "\n",
    "#                 # Extract link for the next page\n",
    "#                 next_link_element = soup.find('a', {'aria-label': 'Halaman berikutnya'})\n",
    "#                 if next_link_element:\n",
    "#                     next_link = next_link_element.get('href')\n",
    "#                 else:\n",
    "#                     print('No next page link found')\n",
    "#                     break\n",
    "#             else:\n",
    "#                 # Subsequent page requests\n",
    "#                 response = requests.get(base_url + next_link)\n",
    "#                 soup = bs4.BeautifulSoup(response.content, features=\"lxml\")\n",
    "#                 next_link_element = soup.find('a', {'aria-label': 'Halaman berikutnya'})\n",
    "#                 if next_link_element:\n",
    "#                     next_link = next_link_element.get('href')\n",
    "#                 else:\n",
    "#                     print('No next page link found')\n",
    "#                     break\n",
    "\n",
    "#             # Extract news data from the current page\n",
    "#             for element in soup.select(\"div > a\"):\n",
    "#                 if str(element.text).endswith('ago'):\n",
    "#                     news = {}\n",
    "#                     # Extracting link, title, source, and timestamp\n",
    "#                     news['link'] = str(element.get('href')).replace(\"/url?q=\", \"\")\n",
    "#                     news['title'] = element.select_one('h3').text\n",
    "#                     for child in element.find_parent().find_all('div', recursive=False):\n",
    "#                         if len(child.select('span')) == 5:\n",
    "#                             news['source'] = child.select('span')[3].text\n",
    "#                             news['timestamp'] = child.select('span')[4].text\n",
    "#                             break\n",
    "\n",
    "#                     # Adding additional information to the news\n",
    "#                     news['search_engine'] = 'google'\n",
    "#                     news['page_count'] = i+1\n",
    "#                     news['search_string'] = keyword\n",
    "#                     # Adding news dictionary to list\n",
    "#                     all_news.append(news)\n",
    "\n",
    "#         # Creating final DataFrame\n",
    "#         final_df = pd.DataFrame(all_news)\n",
    "#         return final_df\n",
    "#     except Exception as e:\n",
    "#         print('Error in Google:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage:\n",
    "# keyword = \"jokowi :: kompas\"\n",
    "# page_count = 2\n",
    "# base_url = \"https://www.google.com/\"\n",
    "\n",
    "# result_df = scrape_google(keyword, page_count, base_url)\n",
    "# print(result_df.head())  # Displaying the first few rows of the DataFram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.google.com/search?q=jokowi::cnn&tbm=nws\n",
      "<Response [429]>\n",
      "Error in Google URL\n",
      "https://www.google.com/search?q=jokowi::cnn&tbm=nws\n",
      "<Response [429]>\n",
      "Error in Google URL\n",
      "https://www.google.com/search?q=jokowi::cnn&tbm=nws\n",
      "<Response [429]>\n",
      "Error in Google URL\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_google(keyword: str, page_count: int, base_url: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        all_news = []\n",
    "        NUMBER_PAGE = page_count\n",
    "        query = keyword\n",
    "        next_link = None  # Initialize next_link to None\n",
    "\n",
    "        # Pagination\n",
    "        for i in range(NUMBER_PAGE):\n",
    "            if next_link is None:\n",
    "                headers = {\n",
    "                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36'\n",
    "                }\n",
    "                # First page request\n",
    "                cek=base_url + \"search?q=\" + query + \"&tbm=nws\"\n",
    "                print(cek)\n",
    "                response = requests.get(cek,headers=headers)\n",
    "                print(response)\n",
    "\n",
    "                if response.status_code != 200:\n",
    "                    print('Error in Google URL')\n",
    "                    continue\n",
    "\n",
    "                soup = bs4.BeautifulSoup(response.content, features=\"lxml\")\n",
    "\n",
    "                # Extract link for the next page\n",
    "                next_link_element = soup.find('a', {'aria-label': 'Halaman berikutnya'})\n",
    "                if next_link_element:\n",
    "                    next_link = next_link_element.get('href')\n",
    "                else:\n",
    "                    print('No next page link found')\n",
    "                    break\n",
    "            else:\n",
    "                # Subsequent page requests\n",
    "                response = requests.get(base_url + next_link,headers=headers)\n",
    "                soup = bs4.BeautifulSoup(response.content, features=\"lxml\")\n",
    "                next_link_element = soup.find('a', {'aria-label': 'Halaman berikutnya'})\n",
    "                if next_link_element:\n",
    "                    next_link = next_link_element.get('href')\n",
    "                else:\n",
    "                    print('No next page link found')\n",
    "                    break\n",
    "\n",
    "            # Extract news data from the current page\n",
    "            for element in soup.select(\"div > a\"):\n",
    "                if str(element.text).endswith('ago'):\n",
    "                    news = {}\n",
    "                    # Extracting link, title, source, and timestamp\n",
    "                    news['link'] = str(element.get('href')).replace(\"/url?q=\", \"\")\n",
    "                    news['title'] = element.select_one('h3').text\n",
    "                    for child in element.find_parent().find_all('div', recursive=False):\n",
    "                        if len(child.select('span')) == 5:\n",
    "                            news['source'] = child.select('span')[3].text\n",
    "                            news['timestamp'] = child.select('span')[4].text\n",
    "                            break\n",
    "\n",
    "                    # Adding additional information to the news\n",
    "                    news['search_engine'] = 'google'\n",
    "                    news['page_count'] = i+1\n",
    "                    news['search_string'] = keyword\n",
    "                    # Adding news dictionary to list\n",
    "                    all_news.append(news)\n",
    "\n",
    "        # Creating final DataFrame\n",
    "        final_df = pd.DataFrame(all_news)\n",
    "        return final_df\n",
    "    except Exception as e:\n",
    "        print('Error in Google:', e)\n",
    "\n",
    "# Example usage\n",
    "keyword = \"jokowi::cnn\"\n",
    "page_count = 3\n",
    "base_url = \"https://www.google.com/\"\n",
    "result_df = scrape_google(keyword, page_count, base_url)\n",
    "print(result_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [429]>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cek='https://www.google.com/search?q=jokowi::cnn&tbm=nws'\n",
    "response = requests.get(cek)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
